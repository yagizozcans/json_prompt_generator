import time
import pandas as pd
import json
import os
from sklearn.metrics import precision_score, recall_score, f1_score
from models.grok_engine import GrokEngine
from models.gemini_engine import GeminiEngine
from utils.helpers import extract_json

TEST_DATA_PATH = "data/test_set.json"

def load_test_data():
    """Loads test data from the JSON file generated by RAGManager."""
    if not os.path.exists(TEST_DATA_PATH):
        # Fallback if file doesn't exist (e.g. first run)
        print(f"Warning: Test data file not found at {TEST_DATA_PATH}. Using fallback.")
        return [
            {"text": "Kƒ±rmƒ±zƒ± bir spor araba √ßiz", "expected_intent": "generate_json"},
            {"text": "CFG Scale nedir?", "expected_intent": "explain_term"}
        ]
    
    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
        return json.load(f)

def evaluate_models(xai_key, google_key, progress_callback=None):
    """
    Evaluates both models using the separate test set (20% split).
    """
    
    # Validation: Ensure keys are present
    if not xai_key or not google_key:
        raise ValueError("API anahtarlarƒ± eksik! L√ºtfen hem xAI hem Google API anahtarlarƒ±nƒ± giriniz.")

    engines = {
        "xAI Grok-2": GrokEngine(xai_key),
        "Google Gemini 2.0 Flash": GeminiEngine(google_key)
    }

    # Load dynamic test data
    test_data = load_test_data()
    results = []
    
    # Stores detailed responses
    responses_log = {i: {} for i in range(len(test_data))}
    
    y_true = [1 if item["expected_intent"] == "generate_json" else 0 for item in test_data]

    total_steps = len(test_data) * len(engines)
    current_step = 0

    print("\nüöÄ --- PERFORMANS TESTƒ∞ BA≈ûLIYOR (Split Test Data) ---")
    print(f"Toplam Test Verisi: {len(test_data)}")

    for model_name, engine in engines.items():
        y_pred = []
        print(f"üîµ [{model_name}] Test Ediliyor...")
        
        for i, item in enumerate(test_data):
            try:
                print(f"   Testing: '{item['text']}' -> ", end="", flush=True)
                
                start_time = time.time()
                # Test data should NOT be in the RAG context (Data Leakage prevention)
                # Ideally, we query RAG for context using the test question
                # But to test pure model capability vs RAG capability, we can choose.
                # Here we will simulate a real user query:
                # 1. Query RAG (which contains only TRAIN data)
                # 2. Feed to LLM
                
                # Note: To avoid circular import, we won't import RAGManager here 
                # but pass empty context or assume app.py handles it.
                # For this evaluation, let's test RAW model capability or with empty context
                # to strictly measure Intent Classification without RAG hints if possible,
                # OR we accept that RAG is part of the system pipeline.
                
                # Decision: Let's test the System Pipeline. 
                # Since we can't easily access RAGManager instance here without passing it,
                # we will use empty context for now to test the "Zero-Shot" intent classification capability.
                # If the goal is testing RAG retrieval + LLM, we'd need the RAG instance.
                
                response = engine.generate_response(item["text"], context_examples=[])
                elapsed = time.time() - start_time
                
                responses_log[i][model_name] = response
                
                json_content = extract_json(response)
                
                if json_content:
                    pred_label = 1 # generate_json
                    print(f"JSON √úretti ({elapsed:.2f}s)")
                else:
                    pred_label = 0 # explain_term / others
                    print(f"Metin √úretti ({elapsed:.2f}s)")
                
                y_pred.append(pred_label)
                    
            except Exception as e:
                print(f"\n   ‚ùå Hata: {e}")
                responses_log[i][model_name] = f"Error: {str(e)}"
                y_pred.append(0)

            current_step += 1
            if progress_callback:
                progress_callback(current_step / total_steps)
        
        # Calculate Metrics
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        results.append({
            "Model": model_name,
            "Precision": round(precision, 4),
            "Recall": round(recall, 4),
            "F1 Score": round(f1, 4)
        })

    print("üèÅ --- TEST Bƒ∞TTƒ∞ --- \n")
    return pd.DataFrame(results), responses_log, test_data
